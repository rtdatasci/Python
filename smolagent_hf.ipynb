{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONdBn8h8aO+tElnTlQzXzT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtdatasci/Python/blob/main/smolagent_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install smolagents"
      ],
      "metadata": {
        "id": "Q1jpb7UtaRjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "Da6bo9ueaFR9",
        "outputId": "4285363a-05f9-49ec-d596-36a5a50c9f43"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mCalculate average of the list [32,34,44,2445\\]\u001b[0m                                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
              "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen3-Next-80B-A3B-Thinking \u001b[0m\u001b[38;2;212;183;2m──────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Calculate average of the list [32,34,44,2445\\]</span>                                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen3-Next-80B-A3B-Thinking ───────────────────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mnumbers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m32\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m34\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m44\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m2445\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34maverage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msum\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnumbers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlen\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnumbers\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m  \n",
              "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34maverage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">numbers </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> [</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">32</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">34</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">44</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">2445</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">average </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> sum(numbers) </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">/</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> len(numbers)</span><span style=\"background-color: #272822\">                                                                          </span>  \n",
              "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(average)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
              " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;212;183;2mFinal answer: 638.75\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: 638.75</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2m[Step 1: Duration 9.73 seconds| Input tokens: 2,044 | Output tokens: 656]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 9.73 seconds| Input tokens: 2,044 | Output tokens: 656]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "638.75"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from smolagents import CodeAgent, InferenceClientModel\n",
        "\n",
        "agent=CodeAgent(\n",
        "    tools=[],\n",
        "    model=InferenceClientModel()\n",
        ")\n",
        "\n",
        "agent.run(\"Calculate average of the list [32,34,44,2445]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add tools to get external data eg. web\n",
        "# many tools available in hf doc: smolagents default_tools or\n",
        "#import specialed tools from hf space hf hub using load_tool by passing repo_id and trust_remove_code=True\n",
        "\n",
        "from smolagents import CodeAgent, InferenceClientModel, WebSearchTool\n",
        "\n",
        "agent=CodeAgent(\n",
        "    tools=[WebSearchTool()],\n",
        "    model=InferenceClientModel()\n",
        ")\n",
        "\n",
        "agent.run(\"what is the height of the tallest mountain in meters?\")\n"
      ],
      "metadata": {
        "id": "TrYir_JQa5lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXltJXiBbi95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add tools to get external data eg. web\n",
        "# many tools available in hf doc: smolagents default_tools or\n",
        "#import specialed tools from hf space hf hub using load_tool by passing repo_id and trust_remove_code=True\n",
        "\n",
        "from smolagents import load_tool\n",
        "\n",
        "model_downloads_tool <- load_tool(\n",
        "    repo_id=\"google/vit-base-patch16-224-in21k\",\n",
        "    trust_remove_code=True\n",
        ")\n",
        "\n",
        "agent=CodeAgent(\n",
        "    tools=[model_downloads_tool, WebSearchTool()],\n",
        "    model=InferenceClientModel()\n",
        ")\n",
        "\n",
        "agent.run(\"Find the most downloaded image classification model in hugging face\")"
      ],
      "metadata": {
        "id": "RUgjsO5Lce1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating custom tools\n",
        "# eg. a custom tool to read a csv using pandas\n",
        "\n",
        "from smolagents import tool\n",
        "\n",
        "# create a tool with @tool decorator\n",
        "@tool\n",
        "def check_inventory(product_name:str) -> int:\n",
        "  \"\"\" check available products in the inventory csv\"\"\"\n",
        "  df= pd.read_csv(\"inventory.csv\")\n",
        "  return df[df[\"product_name\"]==product_name][\"quantity\"]\n",
        "\n",
        "from smolagents import CodeAgent\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[check_inventory], # custom tool here\n",
        "    model=InferenceClientModel(),\n",
        "    additional_authorized_imports=[\"pandas\"]  # allow external packages\n",
        ")\n",
        "\n",
        "agent.run(\"how many large tshirts do you have?\")"
      ],
      "metadata": {
        "id": "QV6A04gXg0My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG\n",
        "\n",
        "!pip install langchain_community\n",
        "!pip install langchain_text_splitters"
      ],
      "metadata": {
        "id": "_MYzZ6rDXFLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader= PyPDFDirectoryLoader(\"cooking_docs\", mode=\"Single\")\n",
        "documents = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        ")\n",
        "\n",
        "chunks = splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "DnTPlAhuXaBG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "OIThptB4WJnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c51af6ad",
        "outputId": "368f8d39-f0c2-41db-9a73-6f4ff4824e1c"
      },
      "source": [
        "!pip install faiss-cpu # for colab env need cpu version to import faiss"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80a97a8"
      },
      "source": [
        "!mkdir -p cooking_docs\n",
        "print(\"Directory 'cooking_docs' created.\")\n",
        "# Now, manually upload your PDF files into the 'cooking_docs' folder using the Colab file browser (folder icon on the left).\n",
        "# After uploading, you can re-run cell 'DnTPlAhuXaBG'."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vector store FAISS (similarity search)\n",
        "# HFendpoint embedding makes sure the query term is matched even if the query term for example error15 in dishwasher might appear in many parts of the manual\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=\"BAAI/bge-base-en-v1.5\",\n",
        "    task=\"feature-extraction\"\n",
        ")\n",
        "\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "## will error out because chunks is empty which is because there is no 'cooking_docs\"\n"
      ],
      "metadata": {
        "id": "CCtomFqaYrPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query the vector store\n",
        "query = \"how do i cook salmon in air fryer?\"\n",
        "\n",
        "# similarity search (get all relevant sections)\n",
        "relevant_docs = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "# create a context string\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n"
      ],
      "metadata": {
        "id": "Ux4xGnjxXDZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agentic rag\n",
        "# retrievel plus reasoning\n",
        "\n",
        "# decorator such as @tool lack memory between queries\n",
        "# use class Tool instead\n",
        "\n",
        "from smolagents import Tool\n",
        "\n",
        "class RecipeSearchTool(Tool):\n",
        "  name=\"recipe_search\",\n",
        "  description=\"searching cooking documentation for recipes and ingredients\",\n",
        "  inputs={\n",
        "      \"query\":{\n",
        "          \"type\":\"string\",\n",
        "          \"description\":\"natural language cooking query\",\n",
        "      }\n",
        "  },\n",
        "  output_type=\"string\",\n",
        "  def __init__(self, vector_store, k=6):  # how many documets to retrieve eg. 6\n",
        "    super().__init__()\n",
        "    self.vector_store = vector_store\n",
        "    self.k = k\n",
        "\n",
        "    # incase no response\n",
        "  def forward(self, query):\n",
        "    docs = self.vector_store.similarity_search(query, k=self.k)\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs) or \"Nothing found\"\n",
        "\n",
        "# create agent with this tool\n",
        "recipe_search = RecipeSearchTool(vector_store)\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[recipe_search],\n",
        "    model=model,\n",
        "    instructions=\"you are a helpful cooking assistant. Be thorough. If initial result is incomplete, try different search terms\",\n",
        "    verbosity_level=1,\n",
        "    max_steps=8\n",
        "\n",
        ")\n",
        "result = agent.run(\"If the AC isn’t cooling and shows error E1, what should I check and what’s the next step?\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "4olJjCMhcn6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#multi-step agent\n",
        "# incoporate planning agent to force a pause and re-think, evaluate what it learned\n",
        "# call back function to make agent do certain custom things after certain steps\n",
        "# callback can be planning (checking) or action (doing) something\n",
        "# callbacks for human checkpoints, checking login status, checking how much token used, etc\n",
        "\n",
        "\n",
        "from smolagents import PlanningStep, ActionStep\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[document_search_tool],\n",
        "    model = model,\n",
        "    step_callbacks= {\n",
        "        PlanningStep: planning_callback,\n",
        "        ActionStep: action_callback\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "UkZae3xViR1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multi agent with multiple specialist\n",
        "# example manager_aget with deepseek reasoning to coordinate, resume_agent with webtools, company_agent with custom tools, etc\n",
        "# manager_agent running maanaged_agents: resume, company, etc\n",
        "\n",
        "\n",
        "# school application agents\n",
        "# School research specialist\n",
        "school_agent = CodeAgent(\n",
        "    # Assign a list of tools the agent can use\n",
        "    tools=[WebSearchTool()],\n",
        "    model=model,\n",
        "    # Set the agent's unique name identifier\n",
        "    name=\"school_research_agent\",\n",
        "    description=\"Expert in researching universities, programs, and admission requirements\"\n",
        ")\n",
        "\n",
        "# Essay writing specialist\n",
        "essay_agent = CodeAgent(\n",
        "    tools=[WebSearchTool()],\n",
        "    # Provide the model used to generate responses\n",
        "    model=model,\n",
        "    name=\"essay_writing_agent\",\n",
        "    # Write a short description of the agent's area of expertise\n",
        "    description=\"Expert in crafting compelling college application essays and personal statements\"\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_vReips4Yh7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# managing memory\n",
        "# each agent.run() starts fresh and has no memory\n",
        "# use reset=False to continue follow up conversation with memory of old results\n",
        "# eg. conversing with booking travel agent\n",
        "# Step 1: Tell the agent your flight date\n",
        "travel_agent.run(\"My Tokyo flight confirmation code is ZX9Q2L.\")\n",
        "\n",
        "# Step 2: Confirm the agent remember when passing the correct reset parameter\n",
        "follow_up = \"What’s my Tokyo flight confirmation code?\"\n",
        "response = travel_agent.run(follow_up, reset=False)\n",
        "\n",
        "print(response)\n",
        "\n",
        "# debug with essay_agent.memory.return_full_code()\n",
        "executed_code = travel_agent.memory.return_full_code()\n",
        "\n",
        "# check what agent is doing essay_agent.memory.get_succinct_steps()\n",
        "# can also save steps to flat json for analysis and regression testing\n",
        "\n"
      ],
      "metadata": {
        "id": "3qD74qQAarl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent answer validation\n",
        "# creating validation rule functions to check eg. response is at least 200 char long\n",
        "# also using another llm to validate with validation prompt \"\"\" reasoning_process{}, agents final answer{}. does the answer round reasonable. respond true or false.\"\"\"\n",
        "\n",
        "def check_answer_length(final_answer, agent_memory):\n",
        "    # Check if answer contains less than 200 characters\n",
        "    if len(str(final_answer)) < 200:\n",
        "        raise Exception(\"The answer is too short. Please include more details.\")\n",
        "    # Return True if check passes\n",
        "    return True\n",
        "\n",
        "\n",
        "real_estate_agent = CodeAgent(\n",
        "    tools=[],\n",
        "    model=model,\n",
        "    # Create the agent with answer length validation\n",
        "    final_answer_checks=[check_answer_length],\n",
        "    verbosity_level=2\n",
        ")\n",
        "\n",
        "# Run the agent with a short prompt\n",
        "response = real_estate_agent.run(\"Suggest a neigborhood for a couple moving to Austin.\")\n",
        "print(response)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "an7Y2K87d0VL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}