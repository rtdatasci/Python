{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzEMJ5L1ie5kNMtREuTi5l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtdatasci/Python/blob/main/finetune_llamafactory_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khLoP5kFRu5W"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "%pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(token = hf_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KeshygqSGVJ",
        "outputId": "4a9b639a-da1b-40f4-f91c-2bfa07027d7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttMJiWVWSQjd",
        "outputId": "119ee0dc-1935-41b0-92a0-b85b6c5926f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-10-04 16:09:10.106201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-04 16:09:10.158571: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-04 16:09:10.173065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-04 16:09:10.201800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-04 16:09:12.035777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://8ac9a863c00e623a37.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-10-04 16:13:33.360186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-04 16:13:33.395875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-04 16:13:33.407093: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-04 16:13:35.037008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/04/2024 16:13:43 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, compute dtype: torch.bfloat16\n",
            "config.json: 100% 1.25k/1.25k [00:00<00:00, 6.72MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-10-04 16:13:44,208 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-04 16:13:44,210 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 50.6k/50.6k [00:00<00:00, 3.80MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 15.3MB/s]\n",
            "special_tokens_map.json: 100% 350/350 [00:00<00:00, 1.75MB/s]\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:46,546 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:46,546 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:46,547 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:46,547 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-10-04 16:13:47,125 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:733] 2024-10-04 16:13:48,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-04 16:13:48,058 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:48,286 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:48,286 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:48,286 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:13:48,286 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-10-04 16:13:48,789 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "10/04/2024 16:13:48 - INFO - llamafactory.data.loader - Loading dataset wiki_qa...\n",
            "Downloading readme: 100% 13.8k/13.8k [00:00<00:00, 30.2kB/s]\n",
            "Downloading data: 100% 594k/594k [00:00<00:00, 695kB/s]\n",
            "Downloading data: 100% 264k/264k [00:00<00:00, 357kB/s]\n",
            "Downloading data: 100% 2.00M/2.00M [00:00<00:00, 2.34MB/s]\n",
            "Generating test split: 100% 6165/6165 [00:00<00:00, 134159.41 examples/s]\n",
            "Generating validation split: 100% 2733/2733 [00:00<00:00, 434831.68 examples/s]\n",
            "Generating train split: 100% 20360/20360 [00:00<00:00, 614006.54 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:01<00:00, 908.47 examples/s] \n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:20<00:00, 49.66 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[35075, 25, 1268, 527, 94867, 66664, 14454, 5380, 72803, 25, 32, 28135, 85177, 94867, 26457, 389, 3700, 6491, 79275, 96486, 662, 128001]\n",
            "inputs:\n",
            "Human: how are glacier caves formed?\n",
            "Assistant:A partly submerged glacier cave on Perito Moreno Glacier.<|end_of_text|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32, 28135, 85177, 94867, 26457, 389, 3700, 6491, 79275, 96486, 662, 128001]\n",
            "labels:\n",
            "A partly submerged glacier cave on Perito Moreno Glacier.<|end_of_text|>\n",
            "[INFO|configuration_utils.py:733] 2024-10-04 16:14:26,428 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-04 16:14:26,429 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "10/04/2024 16:14:26 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[WARNING|quantization_config.py:398] 2024-10-04 16:14:26,854 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 48, in run_sft\n",
            "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 156, in load_model\n",
            "    model = load_class.from_pretrained(**init_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3398, in from_pretrained\n",
            "    hf_quantizer.validate_environment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 62, in validate_environment\n",
            "    raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n",
            "RuntimeError: No GPU found. A GPU is needed for quantization.\n",
            "2024-10-04 16:15:52.043032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-04 16:15:52.084678: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-04 16:15:52.096757: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-04 16:15:54.258000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "10/04/2024 16:16:00 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|configuration_utils.py:733] 2024-10-04 16:16:00,557 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-04 16:16:00,558 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:00,754 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:00,754 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:00,755 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:00,755 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-10-04 16:16:01,243 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:733] 2024-10-04 16:16:02,039 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-04 16:16:02,040 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:02,308 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:02,309 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:02,309 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2269] 2024-10-04 16:16:02,309 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-10-04 16:16:02,760 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "10/04/2024 16:16:02 - INFO - llamafactory.data.loader - Loading dataset wiki_qa...\n",
            "training example:\n",
            "input_ids:\n",
            "[35075, 25, 1268, 527, 94867, 66664, 14454, 5380, 72803, 25, 32, 28135, 85177, 94867, 26457, 389, 3700, 6491, 79275, 96486, 662, 128001]\n",
            "inputs:\n",
            "Human: how are glacier caves formed?\n",
            "Assistant:A partly submerged glacier cave on Perito Moreno Glacier.<|end_of_text|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32, 28135, 85177, 94867, 26457, 389, 3700, 6491, 79275, 96486, 662, 128001]\n",
            "labels:\n",
            "A partly submerged glacier cave on Perito Moreno Glacier.<|end_of_text|>\n",
            "[INFO|configuration_utils.py:733] 2024-10-04 16:16:10,118 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-bnb-4bit/snapshots/941ae13e2e7af21f7109413af7082147f08b7ed7/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-10-04 16:16:10,119 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"unsloth/llama-3-8b-bnb-4bit\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.44.2\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "10/04/2024 16:16:10 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[WARNING|quantization_config.py:398] 2024-10-04 16:16:10,160 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 48, in run_sft\n",
            "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 156, in load_model\n",
            "    model = load_class.from_pretrained(**init_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3398, in from_pretrained\n",
            "    hf_quantizer.validate_environment(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 62, in validate_environment\n",
            "    raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n",
            "RuntimeError: No GPU found. A GPU is needed for quantization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine-tuning the unsloth/llama-3-8b-bnb-4bit model on the Microsoft/wiki_qa dataset\n",
        "\n",
        "4-bit quantized version of the same model provided by Unsloth: select the model name \"Custom\" and paste the repository link \"unsloth/llama-3-8b-bnb-4bit\" into the model path.\n",
        "\n",
        "“Wikiqa” dataset for fine tuning\n",
        "\n",
        "training parameters:\n",
        "Learning rate to 4e-5\n",
        "Epochs to 1.0\n",
        "Max samples to 1000 to speed up the model training"
      ],
      "metadata": {
        "id": "tePyZWESSiuT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVP_xHAWSj8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}