{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6uoB64s5966Fv1OogPJXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtdatasci/Python/blob/main/openai_langchain_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vhk4yIKTbOJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain streamlit openai python-decouple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store your key in colab keys as OPENAI_API_KEY\n",
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "boH18LzvUQgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "O27lDJq-ZmIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write app.py with codes inside\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from decouple import config\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=\"\"\"You are a very kindl and friendly AI assistant. You are\n",
        "    currently having a conversation with a human. Answer the questions\n",
        "    in a kind and friendly tone with some sense of humor.\n",
        "\n",
        "    chat_history: {chat_history},\n",
        "    Human: {question}\n",
        "    AI:\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=config(\"OPENAI_API_KEY\"))\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=4)\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"ChatGPT Clone\",\n",
        "    page_icon=\"ðŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "\n",
        "st.title(\"ChatGPT Clone\")\n",
        "\n",
        "\n",
        "# check for messages in session and create if not exists\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello there, am ChatGPT clone\"}\n",
        "    ]\n",
        "\n",
        "\n",
        "# Display all messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "\n",
        "user_prompt = st.chat_input()\n",
        "\n",
        "if user_prompt is not None:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(user_prompt)\n",
        "\n",
        "\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Loading...\"):\n",
        "            ai_response = llm_chain.predict(question=user_prompt)\n",
        "            st.write(ai_response)\n",
        "    new_ai_message = {\"role\": \"assistant\", \"content\": ai_response}\n",
        "    st.session_state.messages.append(new_ai_message)"
      ],
      "metadata": {
        "id": "PUiGDoPDUmo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the ip address as tunnel password\n",
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "5WzJD7UYZvd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}